{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Bringing in Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Livng\\anaconda3\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "tqdm.pandas(desc=\"Applying\")\n",
    "\n",
    "def train_doc2vec_model(dayfray, name_of_model):\n",
    "    processed = dayfray.apply(lambda x: TaggedDocument(words=(x['doc']), tags=[x['label']]), axis=1)\n",
    "    print('processed')\n",
    "    sents = processed.values\n",
    "    new_model = Doc2Vec(sents, vector_size=300, epochs=40, dm=0, min_count=300)\n",
    "    print('trained')\n",
    "    new_model.save(name_of_model)\n",
    "    print(f'saved {name_of_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Bring in training corpus\n",
    "##### 2. Create X and y\n",
    "##### 3. Train, Test split\n",
    "##### 4. Create dataframes that I can feed to Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"training_documents.csv\", index_col=0)\n",
    "df.head()\n",
    "type(df['doc'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>doc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atensnut</td>\n",
       "      <td>Amazon and Big Tech cozy up to Biden camp with...</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aubrey_huff</td>\n",
       "      <td>After watching the first debate I’m totally co...</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>benshapiro</td>\n",
       "      <td>Alligator pits underneath the podiums https://...</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brieandjam1</td>\n",
       "      <td>Donald #Trump earns his money as a businessman...</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BuzzPatterson</td>\n",
       "      <td>As a traitor to our nation once said, “What di...</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                                                doc label\n",
       "0       atensnut  Amazon and Big Tech cozy up to Biden camp with...   red\n",
       "1    aubrey_huff  After watching the first debate I’m totally co...   red\n",
       "2     benshapiro  Alligator pits underneath the podiums https://...   red\n",
       "3    brieandjam1  Donald #Trump earns his money as a businessman...   red\n",
       "4  BuzzPatterson  As a traitor to our nation once said, “What di...   red"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying: 100%|██████████| 163/163 [01:49<00:00,  1.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['doc'] = df['doc'].progress_apply(lambda x: simple_preprocess(x))\n",
    "type(df['doc'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blue    87\n",
       "red     76\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.533742\n",
      "0    0.466258\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['label'].map({\"red\": 0, \"blue\": 1})\n",
    "X = df['doc']\n",
    "y = df['label']\n",
    "print(y.value_counts(normalize=True))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=32)\n",
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "df_train['doc'] = X_train\n",
    "df_test['doc'] = X_test\n",
    "df_train['label'] = y_train\n",
    "df_test['label'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Doc2Vec on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed\n",
      "trained\n",
      "saved test_doc2vec\n"
     ]
    }
   ],
   "source": [
    "train_doc2vec_model(df_train, 'test_doc2vec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Vectors for training set and testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('test_doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:30<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['vector'] = [model.infer_vector(list(x)) for x in tqdm(df_train['doc'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:22<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test['vector'] = [model.infer_vector(list(x)) for x in tqdm(df_test['doc'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Testing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_test = SVC(probability=True)\n",
    "svc_test.fit(list(df_train['vector']), df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9846153846153847\n",
      "0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "train_hat = svc_test.predict(list(df_train['vector']))\n",
    "print(accuracy_score(df_train['label'], train_hat))\n",
    "test_hat = svc_test.predict(list(df_test['vector']))\n",
    "print(accuracy_score(df_test['label'], test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>label</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[weird, an, idea, sued, judicialwatch, in, cou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5899123, 0.54074347, 0.03056826, -0.0020906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[text, come, to, think, of, it, have, been, no...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04928346, -0.25602424, -0.19530252, -0.2115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[thank, you, for, confirming, what, millions, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.176241, 0.106442586, -0.05736669, -0.068369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[text, gl, balsett, renzograciebjj, classes, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2952346, 0.46000347, 0.05900677, 0.06121755...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[text, did, the, coronavirus, plan, this, even...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.09585878, 0.29221642, 0.085054874, 0.090226...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  doc  label  \\\n",
       "42  [weird, an, idea, sued, judicialwatch, in, cou...      0   \n",
       "0   [text, come, to, think, of, it, have, been, no...      1   \n",
       "24  [thank, you, for, confirming, what, millions, ...      0   \n",
       "0   [text, gl, balsett, renzograciebjj, classes, o...      0   \n",
       "0   [text, did, the, coronavirus, plan, this, even...      1   \n",
       "\n",
       "                                               vector  \n",
       "42  [0.5899123, 0.54074347, 0.03056826, -0.0020906...  \n",
       "0   [0.04928346, -0.25602424, -0.19530252, -0.2115...  \n",
       "24  [0.176241, 0.106442586, -0.05736669, -0.068369...  \n",
       "0   [0.2952346, 0.46000347, 0.05900677, 0.06121755...  \n",
       "0   [0.09585878, 0.29221642, 0.085054874, 0.090226...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9923076923076923\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "logreg_test = LogisticRegression()\n",
    "logreg_test.fit(list(df_train['vector']), df_train['label'])\n",
    "train_hat = logreg_test.predict(list(df_train['vector']))\n",
    "print(accuracy_score(df_train['label'], train_hat))\n",
    "test_hat = logreg_test.predict(list(df_test['vector']))\n",
    "print(accuracy_score(df_test['label'], test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9692307692307692\n",
      "0.7575757575757576\n"
     ]
    }
   ],
   "source": [
    "knn_test = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_test.fit(list(df_train['vector']), df_train['label'])\n",
    "train_hat = knn_test.predict(list(df_train['vector']))\n",
    "print(accuracy_score(df_train['label'], train_hat))\n",
    "test_hat = knn_test.predict(list(df_test['vector']))\n",
    "print(accuracy_score(df_test['label'], test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Attempt at Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "cluster_preds = kmeans.fit_predict(list(df_train['vector']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43846153846153846\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(df_train['label'], cluster_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Final Doc2Vec Model and Final SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed\n",
      "trained\n",
      "saved final_doc2vec\n"
     ]
    }
   ],
   "source": [
    "train_doc2vec_model(df, 'final_doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc2vec = Doc2Vec.load('final_doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/163 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 1/163 [00:00<01:49,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 2/163 [00:01<01:48,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 3/163 [00:02<01:51,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 4/163 [00:02<01:50,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 5/163 [00:03<01:50,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 6/163 [00:04<01:50,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 7/163 [00:04<01:50,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 8/163 [00:05<01:50,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 9/163 [00:06<01:50,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 10/163 [00:07<01:47,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 11/163 [00:07<01:47,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 12/163 [00:08<01:44,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 13/163 [00:09<01:45,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 14/163 [00:09<01:46,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 15/163 [00:10<01:43,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 16/163 [00:11<01:42,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 17/163 [00:12<01:44,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 18/163 [00:12<01:44,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 19/163 [00:13<01:45,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 20/163 [00:14<01:45,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 21/163 [00:14<01:44,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 22/163 [00:15<01:47,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 23/163 [00:16<01:48,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 24/163 [00:17<01:48,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 25/163 [00:18<01:44,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 26/163 [00:18<01:44,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 27/163 [00:19<01:41,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 28/163 [00:20<01:38,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 29/163 [00:21<01:41,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 30/163 [00:21<01:38,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 31/163 [00:22<01:37,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 32/163 [00:23<01:44,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 33/163 [00:24<01:39,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 34/163 [00:24<01:36,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██▏       | 35/163 [00:25<01:34,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 36/163 [00:26<01:32,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 37/163 [00:26<01:30,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 38/163 [00:27<01:29,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 39/163 [00:28<01:26,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 40/163 [00:29<01:26,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 41/163 [00:29<01:24,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 42/163 [00:30<01:24,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 43/163 [00:31<01:23,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 44/163 [00:31<01:21,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 45/163 [00:32<01:21,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 46/163 [00:33<01:20,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 47/163 [00:33<01:19,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 48/163 [00:34<01:22,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 49/163 [00:35<01:20,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 50/163 [00:36<01:21,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 51/163 [00:36<01:19,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 52/163 [00:37<01:17,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 53/163 [00:38<01:17,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 54/163 [00:38<01:16,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▎      | 55/163 [00:39<01:16,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 56/163 [00:40<01:17,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 57/163 [00:41<01:15,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 58/163 [00:41<01:14,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 59/163 [00:42<01:13,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 60/163 [00:43<01:16,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 61/163 [00:43<01:14,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 62/163 [00:44<01:14,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▊      | 63/163 [00:45<01:13,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 64/163 [00:46<01:12,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 65/163 [00:46<01:10,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 66/163 [00:47<01:08,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 67/163 [00:48<01:07,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 68/163 [00:49<01:09,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 69/163 [00:49<01:10,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 70/163 [00:50<01:14,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 71/163 [00:51<01:11,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 72/163 [00:52<01:13,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 73/163 [00:53<01:10,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 74/163 [00:53<00:55,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 75/163 [00:54<00:59,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 76/163 [00:54<00:59,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 77/163 [00:55<01:01,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 78/163 [00:56<01:02,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 79/163 [00:57<01:03,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 80/163 [00:57<01:03,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 81/163 [00:58<01:01,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 82/163 [00:59<00:58,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 83/163 [01:00<00:57,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 84/163 [01:00<00:57,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 85/163 [01:01<01:02,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 86/163 [01:02<01:00,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 87/163 [01:03<00:59,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 88/163 [01:04<00:57,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 89/163 [01:04<00:55,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 90/163 [01:05<00:55,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 91/163 [01:06<00:54,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 92/163 [01:06<00:52,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 93/163 [01:07<00:51,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 94/163 [01:08<00:49,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 95/163 [01:09<00:47,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 96/163 [01:09<00:47,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 97/163 [01:10<00:45,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 98/163 [01:11<00:44,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 99/163 [01:11<00:44,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████▏   | 100/163 [01:12<00:43,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 101/163 [01:13<00:43,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 102/163 [01:14<00:44,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 103/163 [01:14<00:43,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 106/163 [01:15<00:33,  1.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 107/163 [01:16<00:34,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▋   | 108/163 [01:16<00:34,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 109/163 [01:17<00:35,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 110/163 [01:18<00:35,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 111/163 [01:18<00:36,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▊   | 112/163 [01:19<00:35,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 113/163 [01:20<00:35,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 114/163 [01:21<00:35,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 115/163 [01:21<00:34,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 116/163 [01:22<00:33,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 117/163 [01:23<00:34,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 119/163 [01:24<00:27,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 120/163 [01:24<00:28,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 121/163 [01:25<00:27,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 122/163 [01:26<00:27,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 123/163 [01:26<00:26,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 124/163 [01:27<00:26,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 125/163 [01:28<00:26,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 126/163 [01:28<00:25,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 127/163 [01:29<00:25,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▊  | 128/163 [01:30<00:24,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 129/163 [01:31<00:23,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 130/163 [01:31<00:22,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 131/163 [01:32<00:22,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 132/163 [01:33<00:21,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 133/163 [01:33<00:20,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 134/163 [01:34<00:20,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 135/163 [01:35<00:19,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 136/163 [01:35<00:18,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 137/163 [01:36<00:18,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 138/163 [01:37<00:17,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 139/163 [01:38<00:16,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 140/163 [01:38<00:16,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 141/163 [01:39<00:16,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 142/163 [01:40<00:15,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 143/163 [01:41<00:14,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 144/163 [01:41<00:13,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 145/163 [01:42<00:12,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 146/163 [01:43<00:11,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 147/163 [01:43<00:09,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 148/163 [01:44<00:09,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████▏| 149/163 [01:44<00:09,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 150/163 [01:45<00:08,  1.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 151/163 [01:46<00:08,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 152/163 [01:47<00:07,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 153/163 [01:47<00:06,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 154/163 [01:48<00:06,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 155/163 [01:49<00:05,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 156/163 [01:49<00:04,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 157/163 [01:50<00:04,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 158/163 [01:51<00:03,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 160/163 [01:51<00:01,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 161/163 [01:52<00:01,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 162/163 [01:53<00:00,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 163/163 [01:54<00:00,  1.43it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "df['vector'] = [final_doc2vec.infer_vector(list(x)) for x in tqdm(df['doc'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_final = SVC(probability=True)\n",
    "svc_final.fit(list(df['vector']), df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "svc_filename = \"svc_final_model.pkl\"\n",
    "\n",
    "\n",
    "with open(svc_filename, 'wb') as file:\n",
    "    pickle.dump(svc_final, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
